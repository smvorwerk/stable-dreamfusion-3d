# stable-dreamfusion-3d

A colab friendly toolkit to generate 3D mesh model / video / NeRF instance / multiview images of colourful 3D objects by text and image prompts input

Stable Dreamfusion 3D is modified from [dreamfields-torch](https://github.com/ashawkey/dreamfields-torch) and [dreamfields](https://github.com/google-research/google-research/tree/master/dreamfields)

https://user-images.githubusercontent.com/17877083/192129509-eee33042-e88f-4f43-a3e7-9c3a3a5709a4.mp4

Example generated by **text prompt**: "a cyborg organic biological pavilion could breathe with building skin containing algae, in the style of dezeen, trending on artstation, surreal", with CLIP **ViT-L/14** model, training for **200 epochs** with **clip_aug** and **random_fovy_training** mode enabled.

https://user-images.githubusercontent.com/17877083/189866512-d0b96e5d-e40b-4191-b6ed-a3aa833ef1ca.mp4

Example generated by **text prompt**: "a beautiful painting of a flower tree, by Chiho Aoshima, Long shot, surreal", with CLIP **ViT-L/14** model, training for **200 epochs**.

## Main Contributions:
- [x] Export obj & ply model with vertex colour.
- [x] Export  360Â° Video of final model.
- [x] Visualizing the training progress and preview the output video in colab.
- [x] Improve the generation quality.
  - [x] Allow to use different CLIP models.
  - [x] Improve the pre-process of the renderings before feeding into CLIP.
  - [x] Apply random view angle in training.
- [x] Add more useful augments.
- [x] Organize the colab notebook.
## Future update plan:

- [ ] Use different CLIP models simultaneously.
- [ ] Convert existing mesh to NeRF instance then modify by text / image prompts.
- [ ] Reduce GPU RAM occupation in training.

# stable-dreamfusion-3d with pytorch (WIP)

A pytorch implementation of [dreamfields](https://github.com/google-research/google-research/tree/master/dreamfields) as described in [Zero-Shot Text-Guided Object Generation with Dream Fields](https://arxiv.org/abs/2112.01455).

An example of a generated neural field by prompt "cthulhu" viewed in real-time:

https://user-images.githubusercontent.com/25863658/158593558-a52fe215-4276-41eb-a588-cf60c9461cf3.mp4

# Install

The code framework is based on [torch-ngp](https://github.com/ashawkey/torch-ngp).

```bash
git clone https://github.com/svorwerk-dentsu/stable-dreamfusion-3d.git
cd stable-dreamfusion-3d
```

### Install with pip
```bash
pip install -r requirements.txt
```
###  install customized verion of pymarchingcubes
```bash
bash scripts/install_PyMarchingCubes.sh
```

### Build extension
```bash
# install all extension modules
bash scripts/install_ext.sh
# if you want to install manually, here is an example:
cd raymarching
python setup.py build_ext --inplace # build ext only, do not install (only can be used in the parent directory)
pip install . # install to python path (you still need the raymarching/ folder, since this only install the built extension.)
```

# Usage

First time running will take some time to compile the CUDA extensions.

```bash
# text-guided generation
python main_nerf.py --text "cthulhu" --workspace trial --cuda_ray --fp16

# use the GUI
python main_nerf.py --text "cthulhu" --workspace trial --cuda_ray --fp16 --gui

# [experimental] image-guided generation (also use the CLIP loss)
python main_nerf.py --image /path/to/image --workspace trial --cuda_ray --fp16

```

check the `scripts` directory for more provided examples.


# Difference from the original implementation

* Mip-nerf is not implemented, currently only the original nerf is supported.
* Sampling poses with an elevation range in [-30, 30] degrees, instead of fixed at 30 degree.
* Use the origin loss.

